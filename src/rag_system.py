from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain.memory import ConversationBufferWindowMemory  # new import

class RAGSystem:
    def __init__(self, model_name: str):
        self.llm = ChatOpenAI(temperature=0, model_name=model_name, base_url="https://api.proxyapi.ru/openai/v1")
        self.embeddings = OpenAIEmbeddings(base_url="https://api.proxyapi.ru/openai/v1")
        self.memory = ConversationBufferWindowMemory(k=2)  # optimized memory from LangChain
        
    def initialize_from_docs(self, documents):
        self.db = documents  # —Ç–µ–ø–µ—Ä—å documents —ç—Ç–æ —É–∂–µ –≥–æ—Ç–æ–≤–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        self.retriever = self.db.as_retriever(search_kwargs={"k": 5})
        
        prompt = ChatPromptTemplate.from_template('''
            Answer the user's question using only the provided context. 
            If the context does not contain enough information to answer the question, say: 
            "I cannot answer that question because the provided context does not contain relevant information."
            In this case, do NOT include any chapter or page information.
            Do not use any internal knowledge or information outside the provided context.
            Only if you CAN answer the question, after your answer, ON A NEW LINE, include the chapter and page info in this format:
            [–ì–ª–∞–≤–∞ X, –°—Ç—Ä–∞–Ω–∏—Ü–∞ Y]
            For example:
            [–ì–ª–∞–≤–∞ 1, –°—Ç—Ä–∞–Ω–∏—Ü–∞ 27]   
            ALWAYS include at least one direct quote from the text in your answer. Format quotes like this:
            ¬´—Ü–∏—Ç–∞—Ç–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞¬ª
            This is extremely important - your answer MUST include direct quotes from the text!
            Always respond in Russian.
            Conversation History: {history}
            Context: {context}
            Question: {input}
            Answer:
        ''')
        
        document_chain = create_stuff_documents_chain(
            llm=self.llm,
            prompt=prompt
        )
        
        self.retrieval_chain = create_retrieval_chain(
            self.retriever, 
            document_chain
        )
    
    def get_answer(self, question: str) -> str:
        # Load conversation history from optimized memory
        history = self.memory.load_memory_variables({}).get("history", "")
        response = self.retrieval_chain.invoke({'input': question, 'history': history})
        answer = response.get('answer', '–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç —Å–∏—Å—Ç–µ–º—ã.')
        
        # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
        sources = []
        for doc in response.get('source_documents', []):
            chapter = doc.metadata.get('chapter', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –≥–ª–∞–≤–∞')
            page = doc.metadata.get('page', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞')
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–≤–æ–¥–∞ —Å —è—Å–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º –≥–ª–∞–≤—ã –∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            sources.append(f"‚Ä¢ {chapter}, {page}")
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_sources = list(set(sources))[:3]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç—Ä–µ–º—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∫ –æ—Ç–≤–µ—Ç—É
        if unique_sources:
            answer += "\n\nüìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:\n" + "\n".join(unique_sources)
        
        # Update memory with the interaction
        self.memory.chat_memory.add_user_message(question)
        self.memory.chat_memory.add_ai_message(answer)
        return answer